{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "neuralNet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/doodger/dataScience/blob/master/neuralNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZPbL1zkedaN",
        "colab_type": "text"
      },
      "source": [
        "This notebook contains an attempt to implement a neural net from scratch. Mathematical source used is the wikipedia article https://en.wikipedia.org/wiki/Mathematics_of_artificial_neural_networks and https://towardsdatascience.com/https-medium-com-piotr-skalski92-deep-dive-into-deep-networks-math-17660bc376ba , https://towardsdatascience.com/how-to-build-your-own-neural-network-from-scratch-in-python-68998a08e4f6, https://towardsdatascience.com/neural-networks-from-scratch-easy-vs-hard-b26ddc2e89c7, https://towardsdatascience.com/lets-code-a-neural-network-in-plain-numpy-ae7e74410795 , https://github.com/SkalskiP/ILearnDeepLearning.py/blob/master/01_mysteries_of_neural_networks/03_numpy_neural_net/Numpy%20deep%20neural%20network.ipynb\n",
        "http://neuralnetworksanddeeplearning.com/chap2.html\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Atu32cBHeaAm",
        "colab_type": "code",
        "outputId": "c7827ebe-e3a5-47f6-cfb6-a7d1668a5a93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "\n",
        "def f_reLU(x):\n",
        "  #implementation of the rectified function\n",
        "  return x if x > 0 else 0\n",
        "\n",
        "#old def f_sigmoid(x):\n",
        "  #implementation of the sigmoid function\n",
        "#  return ((np.exp(x))/(1+np.exp(x)))\n",
        "def f_sigmoid(x):\n",
        "    return 1.0/(1+ np.exp(-x))\n",
        "\n",
        "\n",
        "class neuralNet:\n",
        "  \n",
        "  def __init__(self,inputV=[],hidden=[],output=[], b = [], activationF=[]):\n",
        "  # inputV is an entry vector containing the x training data\n",
        "  # hidden is a numpy array of arrays of length n for the n layers\n",
        "  # output is the target output or \"training y\"\n",
        "  # activationF contains the function used for each layer\n",
        "    self.x = inputV\n",
        "    self.w = hidden\n",
        "    self.y = output\n",
        "    self.a = [0]*len(hidden)\n",
        "    self.b = b\n",
        "    self.z = [0]*len(hidden)\n",
        "    self.activationF = activationF\n",
        "    self.fDictio = {\n",
        "           \"relu\" : f_reLU,\n",
        "           \"sigmoid\" : f_sigmoid,\n",
        "        }\n",
        "  \n",
        "  def feedForward(self):\n",
        "    #the first layer uses the input vector instead of the previous layers\n",
        "    print(self.activationF[0])\n",
        "    self.z[0] = self.w[0]@self.x #TODO this requires the b layer as well\n",
        "    self.a[0] = self.fDictio[self.activationF[0]](self.z[0])\n",
        "    print(\"layer 0 : \",self.a[0])\n",
        "    for i in range(1,len(hidden)):\n",
        "      self.z[i] = self.w[i]@self.a[i-1]#np.dot(self.w[i],self.a[i-1]) #+self.b[i]\n",
        "      self.a[i] = self.fDictio[self.activationF[i]](self.z[i])\n",
        "    self.output = self.a[len(hidden)-1]\n",
        "\n",
        "  def backPropagation(self):\n",
        "  #for every layer, calculate recursively a chain of derivative\n",
        "  #starting with derivative of loss function wrt y hat\n",
        "    outputError = self.lossFunction(self.output,self.y)\n",
        "    for i in reversed(range(1,len(a))):\n",
        "      print(\"bar\")\n",
        "\n",
        "  def lossFunction(self,yHat,y):\n",
        "  #calculates the loss. Right now only MSE\n",
        "  #TODO maybe have this use two vectors as input\n",
        "  #TODO maybe add in other functions\n",
        "    self.loss = (1/2*self.y.shape[0])*np.linalg.norm(self.y-self.output)\n",
        "    print(self.loss)\n",
        "#xInput = np.array([[0,0,1],\n",
        "#                   [0,1,1],\n",
        "#                   [1,0,1],\n",
        "#                   [1,1,1]])\n",
        "#yTarget = np.array([0,1,1,0])\n",
        "hidden = [\n",
        "  np.random.rand(25,2),\n",
        "  np.random.rand(50,25),\n",
        "  np.random.rand(50,50),\n",
        "  np.random.rand(25,50),\n",
        "  np.random.rand(1,25)\n",
        "]\n",
        "activation = [\"relu\",\"relu\",\"relu\",\"relu\",\"sigmoid\"]\n",
        "X, y = make_moons(n_samples = 1000, noise=0.2, random_state=100)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "\n",
        "print(activation[0])\n",
        "testNet = neuralNet(np.transpose(X_train),hidden,np.transpose(y_train),activationF = activation)\n",
        "testNet.feedForward()\n",
        "#testNet.backPropagation()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "relu\n",
            "relu\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-cb5cff9d178d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0mtestNet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneuralNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivationF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m \u001b[0mtestNet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeedForward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;31m#testNet.backPropagation()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-cb5cff9d178d>\u001b[0m in \u001b[0;36mfeedForward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivationF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;31m#TODO this requires the b layer as well\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfDictio\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivationF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"layer 0 : \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-cb5cff9d178d>\u001b[0m in \u001b[0;36mf_reLU\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mf_reLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;31m#implementation of the rectified function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#old def f_sigmoid(x):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxgpyaETyh5H",
        "colab_type": "code",
        "outputId": "f15a5d9e-50d1-4020-f430-77fb77abfd7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "ab = np.array([[1,1],[2]])\n",
        "print(ab)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[list([1, 1]) list([2])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_PSHUylcRDf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_jCsZ3TcRBT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fts_A2SfxlKs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A19awxBkaCqa",
        "colab_type": "text"
      },
      "source": [
        "TODO: generalize further, modifiable structure, different functions, layer*weight switcharoo\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qP3wjoZpjRiS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}