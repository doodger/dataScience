{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "neuralNet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/doodger/dataScience/blob/master/neuralNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZPbL1zkedaN",
        "colab_type": "text"
      },
      "source": [
        "This notebook contains an attempt to implement a neural net from scratch. Mathematical source used is the wikipedia article https://en.wikipedia.org/wiki/Mathematics_of_artificial_neural_networks and https://towardsdatascience.com/https-medium-com-piotr-skalski92-deep-dive-into-deep-networks-math-17660bc376ba , https://towardsdatascience.com/how-to-build-your-own-neural-network-from-scratch-in-python-68998a08e4f6, https://towardsdatascience.com/neural-networks-from-scratch-easy-vs-hard-b26ddc2e89c7, https://towardsdatascience.com/lets-code-a-neural-network-in-plain-numpy-ae7e74410795 , https://github.com/SkalskiP/ILearnDeepLearning.py/blob/master/01_mysteries_of_neural_networks/03_numpy_neural_net/Numpy%20deep%20neural%20network.ipynb\n",
        "http://neuralnetworksanddeeplearning.com/chap2.html\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Atu32cBHeaAm",
        "colab_type": "code",
        "outputId": "3ef18b42-eccf-4fb0-8e53-b49c5ebb6283",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def f_reLU(x):\n",
        "  #implementation of the rectified function\n",
        "  return x if x > 0 else 0\n",
        "\n",
        "#old def f_sigmoid(x):\n",
        "  #implementation of the sigmoid function\n",
        "#  return ((np.exp(x))/(1+np.exp(x)))\n",
        "def f_sigmoid(x):\n",
        "    return 1.0/(1+ np.exp(-x))\n",
        "\n",
        "\n",
        "class neuralNet:\n",
        "  \n",
        "  def __init__(self,inputV=[],hidden=[],output=[], b = []):\n",
        "  # inputV is an entry vector\n",
        "  # hidden is a numpy array of arrays of length n for the n layers\n",
        "  # output is the target output or \"true y\"\n",
        "    self.x = inputV\n",
        "    self.w = hidden\n",
        "    self.y = output\n",
        "    self.a = [0]*len(hidden)\n",
        "    self.b = b\n",
        "    self.z = [0]*len(hidden)\n",
        "\n",
        "  def feedForward(self):\n",
        "    #the first layer uses the input vector instead of the previous layers\n",
        "    self.z[0] = self.w[0]@self.x #np.dot(self.w[0],self.x) #+self.b[0]\n",
        "    self.a[0] = f_sigmoid(self.z[0])\n",
        "    print(\"layer 0 : \",self.a[0])\n",
        "    for i in range(1,len(hidden)):\n",
        "      self.z[i] = self.w[i]@self.a[i-1]#np.dot(self.w[i],self.a[i-1]) #+self.b[i]\n",
        "      self.a[i] = f_sigmoid(z[i])\n",
        "    self.output = self.a[len(hidden)-1]\n",
        "\n",
        "  def backPropagation(self):\n",
        "  #for every layer, calculate recursively a chain of derivative\n",
        "  #starting with derivative of loss function wrt y hat\n",
        "    print(\"blep\")\n",
        "\n",
        "  def lossFunction(self):\n",
        "  #calculates the loss. Right now only MSE\n",
        "    costOut = (1/2*self.y.shape[0])*np.linalg.norm(self.y-self.output)\n",
        "    print(\"bloop\")\n",
        "\n",
        "xInput = np.array([[0,0,1],\n",
        "                   [0,1,1],\n",
        "                   [1,0,1],\n",
        "                   [1,1,1]])\n",
        "yTarget = np.array([0,1,1,0])\n",
        "hidden = [\n",
        "  np.random.rand(4,3),\n",
        "  np.random.rand(3,1)\n",
        "]\n",
        "print(len(hidden))\n",
        "testNet = neuralNet(xInput,hidden,yTarget)\n",
        "testNet.feedForward()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n",
            "layer 0 :  [[0.81269175 0.85467951 0.95183194]\n",
            " [0.53397604 0.61709633 0.72060923]\n",
            " [0.71127097 0.73605542 0.86114791]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-dc3f23ae2adc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0mtestNet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneuralNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxInput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myTarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mtestNet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeedForward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-dc3f23ae2adc>\u001b[0m in \u001b[0;36mfeedForward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"layer 0 : \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;31m#np.dot(self.w[i],self.a[i-1]) #+self.b[i]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf_sigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 3 is different from 1)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxgpyaETyh5H",
        "colab_type": "code",
        "outputId": "f15a5d9e-50d1-4020-f430-77fb77abfd7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "ab = np.array([[1,1],[2]])\n",
        "print(ab)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[list([1, 1]) list([2])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fts_A2SfxlKs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A19awxBkaCqa",
        "colab_type": "text"
      },
      "source": [
        "TODO: generalize further, modifiable structure, different functions, layer*weight switcharoo\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qP3wjoZpjRiS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}